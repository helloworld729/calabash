<font color=#FF0000>**1、什么是特征工程？**:</font>

**特征工程**，顾名思义，是对原始数据进行一系列**工程处理**，将其提炼为特征，作为输入供算法和模型使用。<br>
从本质上来讲，特征工程是一个表示和展现数据的过程。在实际工作中，特征工程旨在**去除原始数据中的<br>
杂质和冗余**，设计更高效的特征以刻画求解的问题与预测模型之间的关系。<br>


<font color=#FF0000>**2、进行归一化原因和方法？**:</font>

对于几个机器学习问题，模型参数往往是多维的，一般而言学习率对不同的维度是统一的，这样一来往往<br>
**变化范围较大的的特征会主导**步进过程，干扰了其他维度特征的学习。所以先对数据做归一化处理，这是<br>
为了**提高模型收敛速度**。<br>
另外需要指出的是：归一化不是万能的，一般而言，采用梯度下降的算法需要进行归一化，但是决策树不<br>
需要，因为是否归一化对信息增益没有影响。<br>
具体的方法有：**线型归一化和零均值归一化**。相关图形和计算方法如下：<br>

<font color=#FF0000>**3、词嵌入模型如何工作？**:</font>

**CBOW:**根据上下文来预测当前词的生成概率。假设语料库有N个单词<br>
包括：<br>
**输入层**：多个语境单词的上下文one-hot编码<br>
**映射层**: 隐层平均值<br>
**输出层**: N维向量，然后经过激活函数，得到中心词在语料库上的概率分布，最后与true_label构建交叉熵损失函数进行迭代。<br>
**问题与改进**：用softmax激活的时候，反向传播需要考虑所有的单词，导致收敛速度慢，所以产生了层次softmax和负采样技术，<br>
&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;负采样就是对负样本进行采样，未被采样的单词不参与计算，采样概率和词频正比。<br>

<font color=#FF0000>**4、word2vec与LDA的联系分析？**:</font>

LDA是基于词袋的概率模型，通过吉普斯采样收敛后可以获得文档-主题、主题-单词的概率分布。word2vec是利用深度学习<br>
得到词汇的稠密向量。两者从本质上都利用了词汇的共现信息。假如对于文档词汇的词向量做聚类分析，也可以得到文档的主题/关键词。<br>


```python

```
