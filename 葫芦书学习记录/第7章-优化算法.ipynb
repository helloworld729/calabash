{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数刻画了模型和训练样本的匹配程度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1、常用的损失函数**：<br>\n",
    "分类问题：交叉熵损失函数<br>\n",
    "回归问题：均方误差损失函数(异常点敏感)、绝对值损失(不可导)、Huber损失(两者的结合)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、随机梯度下降与优化算法<br>\n",
    "经典的梯度下降算法需要遍历所有的数据进行一次迭代，当数据量很大时，这基本不可行。<br>\n",
    "随机梯度下降法用单个样本更新参数，大大加快了熟练速率。<br>\n",
    "为了降低随机梯度下降算法的方差，提高算法稳定性，充分利用计算资源和矩阵操作，实际应用中<br>\n",
    "多采用批量梯度下降算法。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、随机梯度下降的失效和改进<br>\n",
    "深度学习的优化本来就是一个很难的问题，例如普遍存在的局部最优问题，但对于随机梯度下降法更困难的问题是鞍点和狭谷。<br>\n",
    "狭谷：来回反弹，导致收敛不稳定或者收敛速度很慢；<br>\n",
    "鞍点：一个方向沿两端上升，另一个方向沿两端下降，中间是较为平坦的区域；造成梯度消失。<br>\n",
    "**解决方法1**：动量法，虽然叫动量法，但实际上是模拟物理学速度的更新，V=V0+at，使得当前的速度不仅受到角速度的影响，也受到记忆的影响。<br>\n",
    "**解决方法2**：Adgrad方法，如果说动量发是速度保持，那Adgrad就是环境感知，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
